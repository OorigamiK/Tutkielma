\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, url, hyperref}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{cleveref}
\usepackage[thinc]{esdiff}
\usepackage{commath}
\usepackage{xr-hyper}
\externaldocument{appendix}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\title{Asymptotic Behavior of the Number of Block Sums}
\author{Ohto Katila}
\date{\today}


\let\epsilon\varepsilon

\begin{document}
	\maketitle

	\begin{abstract}
		In this paper we extend the paper of O'Sullivan et al. on block sums of prime powers to regularly varying functions under mild additional assumptions. We find an exact asymptotic for the number of block sums below $x$. This paper applies tools from the theory of regularly varying functions and relates the number of blocks of length $i$ to $\phi(x)=(1+x^{\alpha+1})^{\frac{1}{\alpha+1}}-x$.
	\end{abstract}
	\section{Introduction}
	Determining the asymptotic behavior of the count of block sums less than $x$ has been investigated for primes and prime powers \cite{Moser_1963}\cite{primePowerConsecutiveSums}\cite{primeSquereConsecutiveSums}. However the asymptotic for more general sequences remains unknown. In this paper we find asymptotic formula for class of regularly varying sequences, that are bounded away from 0 and $\infty$ in every closed interval and eventually monotonic with index $\alpha\neq 1, \alpha>0$.
	
	Our main theorem provides an asymptotic for the block sum counting function
	\begin{align}
	S(x)=\#\{(i, j)\in\mathbb{N}^2\mid i\le j, f(i)+f(i+1)+\dots+f(j)\le x\}.
	\end{align}
	That is 
	\begin{align}
	S(x)\sim n^2C(\alpha), n\to\infty
	\end{align}
	where $x\sim \frac{nf(n)}{\alpha+1}, x\to\infty$ and 
	\begin{align}
	C(\alpha)=\frac{1}{2(\alpha+1)}
	\frac{\Gamma(\frac{1}{\alpha+1})\Gamma(\frac{\alpha-1}{\alpha+1})}{\Gamma(\frac{\alpha}{\alpha+1})}
	\end{align} 
	for a regularly varying function $f$ with index $\alpha>1$ that satisfies certain assumptions.
	
	The proof relies on classical results from the theory of regular variation, in particular the uniform convergence theorem and Potter's bound \cite{bingham1987regular}. We can split the $S(x)$ into sum of $s_i$ that count the number of blocks of length $i$. The main idea in the proof is that we can relate $s_i$ to the inverse of 
	\begin{align}
	\phi(x)=(1+x^{\alpha+1})^{\frac{1}{\alpha+1}}
	\end{align} 
	which allows us to calculate the exact coefficient in the asymptotic.
	
	
	This paper is organized as follows. Section \ref{notation} is on notation. Section \ref{preliminaries} first proves basic lemmas about asymptotics, then lemma that calculates $\phi$'s inverses integral and finally lemma that relates $\phi$ and $s_i$. Section \ref{mainTheorem} proves the main theorem \ref{BlockSumsforAlphaGr1} and one technical lemma bounding sum of $s_i$ for small $i$'s.
	
	\section{Notation}\label{notation}
	\begin{itemize}
		\item Let $RV_\alpha^+$ denote eventually monotonic regularly varying functions bounded away from $0$ and $\infty$ in any closed interval with index $\alpha$ (see \cite{bingham1987regular}). We assume the domain and range are $\mathbb{R}_{>0}$.
		\item $S(x)=\#\{(i, j)\in\mathbb{N}^2\mid i\le j, f(i)+f(i+1)+\dots+f(j)\le x\}$ – the number of block sums below $x$.
		\item $s_k(x)=\#\{(i, j)\mid j-i=k-1, f(i)+f(i+1)+\dots+f(j)\le x\}$ – the number of block sums of length $k$ below $x$.
		\item  $f(x)\sim g(x), x\to a$ – meaning $\lim_{x\to a} \frac{f(x)}{g(x)} = 1$.
		We often omit the "$x\to a$" part, when it is clear from context.
		\item $f(x)\lesssim g(x)$ – meaning $\limsup\frac{f(x)}{g(x)}\le1$
		\item $f(x)\gtrsim g(x)$ – meaning $\liminf\frac{f(x)}{g(x)}\ge 1$.
		\item We denote sums of the form
		\begin{align}
		\sum_{x\le i< y} F(i)
		\end{align}
		with
		\begin{align}
		\sum_{i=x}^{y} F(i).
		\end{align}
		\item $f(x)=O(g(x))$ – there exists $C>0$ such that $\abs{f(x)}\le Cg(x)$ for all sufficiently large $x$.
		\item $f(x)=o(g(x))$ – shorthand for
		\[
		\lim_{x\to\infty} \frac{f(x)}{g(x)}=0
		\]
		
	\end{itemize}
	
	\section{Preliminaries}\label{preliminaries}
	We assume throughout this paper that $n=n(x)$ is the unique integer satisfying
	\begin{align}
	\sum_{i=1}^{n} f(i)\le x<\sum_{i=1}^{n+1} f(i)
	\end{align}
	where $f\in RV_\alpha^+$ that is $f(x)=x^\alpha L(x)$ (see section 1.5 in \cite{bingham1987regular}), for some slowly varying function $L$.
	
	\begin{lemma}\label{sum f Asymp}
		We have
		\begin{align}
		x\sim \frac{n^{\alpha+1}L(n)}{\alpha+1}=\frac{nf(n)}{\alpha+1}
		\end{align}
	\end{lemma}
	\begin{proof}
		Let $\epsilon>0$ and split the sum
		\begin{align}
		\sum_{i=1}^{n} f(i)=\sum_{i=1}^{n\epsilon} f(i)+\sum_{i=n\epsilon}^{n} f(i).
		\end{align}
		We will first examine the second sum.
		\begin{align}
		\sum_{i=n\epsilon}^{n} f(i)=\sum_{i=n\epsilon}^{n} i^\alpha L(i)
		\end{align}
		Using uniform convergence theorem (see Theorem 1.2.1 in \cite{bingham1987regular} or  Theorem~\ref{UCT} in the Appendix)
		\begin{align}
			\sum_{i=n\epsilon}^{n} i^\alpha L(i)&\sim L(n)\sum_{i=n\epsilon}^{n} i^\alpha\\
			&\sim L(n)\int_{n\epsilon}^{n} t^\alpha dt\\
			&=\frac{1}{\alpha+1}L(n)n^{\alpha+1}(1-\epsilon^{\alpha+1})
		\end{align}
		Examining the first sum
		\begin{align}
		\sum_{i=1}^{n\epsilon} f(i)<n\epsilon f(n\epsilon).
		\end{align}
		Since $f$ is eventually increasing
		\begin{align}
		n\epsilon f(n\epsilon)< n\epsilon f(n)=\epsilon n^{\alpha+1}L(n).
		\end{align}
		Combining these we get
		\begin{align}
			\frac{1}{\alpha+1}n^{\alpha+1}L(n)(1-\epsilon^{\alpha+1})\lesssim\sum_{i=1}^{n} f(i)&\lesssim \frac{1}{\alpha+1}n^{\alpha+1}L(n)(1+\epsilon(\alpha+1)-\epsilon^{\alpha+1})\\
			\sum_{i=1}^{n} f(i)&\sim \frac{1}{\alpha+1}n^{\alpha+1}L(n)
		\end{align}
	\end{proof}
	
	\begin{lemma}\label{sum f range asymp}
		Let $\epsilon>0$ given $k,l>0$ such that $k-l>\epsilon$. If 
		\begin{align}
		\sum_{i=nl}^{nk} f(i)<x
		\end{align} 
		then
		\begin{align}
		\sum_{i=nl}^{nk} f(i)\sim \frac{1}{\alpha+1}L(n)n^{\alpha+1}(k^{\alpha+1}-l^{\alpha+1}).
		\end{align}
	\end{lemma}
	\begin{proof}
		From the definition of $n$ it follows $k\le l+1$. Since $f$ is eventually increasing
		\begin{align}
		\sum_{i=nl}^{nk} f(i)>n(k-l) f(nl)>n\epsilon f(nl).
		\end{align}
		Using Potter's bound (see Theorem 1.5.6 \cite{bingham1987regular} or Theorem~\ref{PottersBound} in the Appendix) and Lemma~\ref{sum f Asymp} %laita potter ja lemma 1 eri kohtiin
		\begin{align}
			\frac{1}{\alpha+1}n^{\alpha+1}L(n)&\gtrsim n\epsilon f(nl)\\
			&= n^{\alpha+1}\epsilon l^{\alpha}L(nl)\\
			&\gtrsim \epsilon n^{\alpha+1}L(n)l^{\alpha}\frac{1}{2\max(l, l^{-1})}.
		\end{align}
		Thus
		\begin{align}
		l\lesssim \max\left({(\frac{2}{\epsilon(\alpha+1)})^{\frac{1}{\alpha+1}}}, {(\frac{2}{\epsilon(\alpha+1)})^{\frac{1}{\alpha-1}}}\right).
		\end{align}
		Now $l$ and $k$ are bounded above and below, so  we can use uniform convergence theorem (see Theorem 1.2.1 in \cite{bingham1987regular} or Theorem~\ref{UCT} in the Appendix)
		\begin{align}
			\sum_{i=nl}^{nk} f(i) &\sim \sum_{i=nl}^{nk} i^\alpha L(n)\\
			&=L(n)\sum_{i=nl}^{nk} i^\alpha\\
			&\sim L(n)\int_{nl}^{nk} t^\alpha dt\\
			&=\frac{1}{\alpha+1}L(n)n^{\alpha+1}(k^{\alpha+1}-l^{\alpha+1}).
		\end{align}
	\end{proof}
	
	\begin{lemma}\label{phi integral}
		The function $\phi:(0,\infty)\to(0,1)$, $\phi(x)=(1+x^{\alpha+1})^{\frac{1}{\alpha+1}}-x$ is strictly decreasing bijection hence has inverse and
		\begin{align}
		\int_{0}^{1} \phi^{-1}(t)dt=\frac{1}{2(\alpha+1)}
		\frac{\Gamma(\frac{1}{\alpha+1})\Gamma(\frac{\alpha-1}{\alpha+1})}{\Gamma(\frac{\alpha}{\alpha+1})}
		\end{align}
		for $\alpha>1$.
	\end{lemma}
	
	\begin{proof}
		Taking the derivative of $\phi$
		\begin{align}
			\diff{\phi}{x}&=(1+x^{\alpha+1})^{-\frac{\alpha}{\alpha+1}}x^{\alpha}-1\\
			&=\frac{x^\alpha}{((1+x^{\alpha+1})^{\frac{1}{\alpha+1}})^{\alpha}}-1\\
			&<1-1=0
		\end{align}
		where last inequality follows from $(1+x^{\alpha+1})^{\frac{1}{\alpha+1}}>x$. Because $\phi$'s derivative is negative everywhere, $\phi$ is strictly decreasing. Now $\phi$ is bijection, because $\phi(0)=1$ and $\lim_{x\to\infty}\phi(x)=0$, hence has inverse. Since $\phi$ is strictly decreasing bijection
		\begin{align}
		\int_{0}^{1} \phi^{-1}(x)dx=\int_{0}^{\infty} \phi(x)dx.
		\end{align}
		Next we show that the integral converges. Since $x^{\frac{1}{\alpha+1}}$ is increasing with decreasing derivative, we have $(1+y)^{\frac{1}{\alpha+1}}<y^{\frac{1}{\alpha+1}}+\frac{1}{\alpha+1}y^{-\frac{\alpha}{\alpha+1}}$. Using this we get
		\begin{align}
		\phi(x)<x^{-\alpha},
		\end{align}
		so the integral converges.
		
		Doing change of variables $t=\frac{x^{\alpha+1}}{1+x^{\alpha+1}}$, $x=(\frac{t}{1-t})^{\frac{1}{\alpha+1}}$. We get
		\begin{align}
		dx=\frac{1}{\alpha+1}\big(\frac{t}{1-t}\big)^{-\frac{\alpha}{\alpha+1}}(1-t)^{-2}dt.
		\end{align}
		Changing $\phi(x)$ to $t$ terms \\
		\begin{align}
			\phi(x)&=(1+x^{\alpha+1})^{\frac{1}{\alpha+1}}-x  \\
			&=(\frac{1}{1-t})^{\frac{1}{\alpha+1}}-(\frac{t}{1-t})^{\frac{1}{\alpha+1}} 	\\
			&=(1-t)^{-\frac{1}{\alpha+1}}(1-t^{\frac{1}{\alpha+1}}).
		\end{align}
		Putting it together
		\begin{align}
			\int_{0}^{\infty} \phi(x)dx&=\int_{0}^{1} (1-t)^{-\frac{1}{\alpha+1}}(1-t^{\frac{1}{\alpha+1}})
			\frac{1}{\alpha+1}\big(\frac{t}{1-t}\big)^{-\frac{\alpha}{\alpha+1}}(1-t)^{-2}dt\\
			&=\frac{1}{\alpha+1}\int_{0}^{1} (1-t)^{\frac{\alpha}{\alpha+1}-\frac{1}{\alpha+1}-2}
			(t^{-\frac{\alpha}{\alpha+1}}-t^{\frac{1}{\alpha+1}-\frac{\alpha}{\alpha+1}})dt\\
			&=\frac{1}{\alpha+1}\int_{0}^{1} (1-t)^{-\frac{2}{\alpha+1}-1}
			(t^{-\frac{\alpha}{\alpha+1}}-t^{\frac{1-\alpha}{\alpha+1}})dt
		\end{align}
		
		
		
		Let $g_r(t)=(1-t)^{r-\frac{2}{\alpha+1}-1}
		(t^{-\frac{\alpha}{\alpha+1}}-t^{\frac{1-\alpha}{\alpha+1}})$ and $g(t)=(1-t)^{-\frac{2}{\alpha+1}-1}
		(t^{-\frac{\alpha}{\alpha+1}}-t^{\frac{1-\alpha}{\alpha+1}})$. Since for each $r>0$, $g_r(t)\to g(t), r\to0$ pointwise
		and $\abs{g_r(t)}<\abs{g(t)}$, we can use dominated convergence theorem. Thus
		\begin{align}
			\int_{0}^{1} (1-t)^{-\frac{2}{\alpha+1}-1}
			(t^{-\frac{\alpha}{\alpha+1}}-t^{\frac{1-\alpha}{\alpha+1}})dt\\
			=\lim_{r\to0^+} \int_{0}^{1} (1-t)^{r-\frac{2}{\alpha+1}-1}
			(t^{-\frac{\alpha}{\alpha+1}}-t^{\frac{1-\alpha}{\alpha+1}})dt.
		\end{align}
		
		Let $B$ be the beta function (see NIST DLMF § 5.12.1 \cite{NIST:DLMF})
		\begin{align}
		B(z_1, z_2)=\int_{0}^{1} t^{z_1-1}(1-t)^{z_2-1}dt=\frac{\Gamma(z_1)\Gamma(z_2)}{\Gamma(z_1+z_2)}
		\end{align}
		The beta function can be analytically extended to $\left(\mathbb{C}\setminus \mathbb{Z},\mathbb{C}\setminus \mathbb{Z}\right)$ using Pochhammer's integral (see NIST DLMF § 5.12.12 \cite{NIST:DLMF}).
		%give more arguments for the analytic continuation
		%show explicitly that there is the required region.
		\begin{align}
			&=\frac{1}{\alpha+1}\int_{0}^{1} (1-t)^{-\frac{2}{\alpha+1}-1}
			(t^{-\frac{\alpha}{\alpha+1}}-t^{\frac{1-\alpha}{\alpha+1}})dt\\
			&=\frac{1}{\alpha+1} \lim_{r\to0^+} \int_{0}^{1} (1-t)^{r-\frac{2}{\alpha+1}-1}
			(t^{-\frac{\alpha}{\alpha+1}}-t^{\frac{1-\alpha}{\alpha+1}})dt\\
			&=\frac{1}{\alpha+1}\lim_{r\to 0^+}\left(B\left(\frac{1}{\alpha+1}, r-\frac{2}{\alpha+1}\right)-
			B\left(\frac{2}{\alpha+1}, r-\frac{2}{\alpha+1}\right)\right).
		\end{align}
		Using gamma expression for beta functions we get
		\begin{align}
			\int_{0}^{\infty} \phi(x)dx&=\frac{1}{\alpha+1}\lim_{r\to 0^+}\left(
			\frac{\Gamma(\frac{1}{\alpha+1})\Gamma(r-\frac{2}{\alpha+1})}{\Gamma(\frac{1}{\alpha+1}+r-\frac{2}{\alpha+1})}
			-
			\frac{\Gamma(\frac{2}{\alpha+1})\Gamma(r-\frac{2}{\alpha+1})}{\Gamma(\frac{2}{\alpha+1}+r-\frac{2}{\alpha+1})}
			\right)\\
			&=\frac{1}{\alpha+1}\lim_{r\to 0^+}\left(
			\frac{\Gamma(\frac{1}{\alpha+1})\Gamma(-\frac{2}{\alpha+1})}{\Gamma(-\frac{1}{\alpha+1})}
			-
			\frac{\Gamma(\frac{2}{\alpha+1})\Gamma(-\frac{2}{\alpha+1})}{\Gamma(r)}
			\right).
		\end{align}
		Since $\Gamma(t)=\frac{\Gamma(1+t)}{t}$ we have $\Gamma(t)\sim\frac{1}{t}, t\to 0$. Thus
		\begin{align}
		\lim_{r\to 0}\frac{\Gamma(\frac{2}{\alpha+1})\Gamma(-\frac{2}{\alpha+1})}{\Gamma(r)}=0
		\end{align}
		so the expression simplifies to
		\begin{align}
			\int_{0}^{\infty} \phi(x)dx&=\frac{1}{\alpha+1}
			\frac{\Gamma(\frac{1}{\alpha+1})\Gamma(-\frac{2}{\alpha+1})}{\Gamma(-\frac{1}{\alpha+1})}
			\\
			&=\frac{1}{2(\alpha+1)}
			\frac{\Gamma(\frac{1}{\alpha+1})\Gamma(\frac{\alpha-1}{\alpha+1})}{\Gamma(\frac{\alpha}{\alpha+1})}
		\end{align}
		where last equality uses $z\Gamma(z)=\Gamma(z+1)$.
	\end{proof}
	
	\begin{lemma}\label{s asymp}
		Let $\epsilon>0$ and $i>\epsilon n$, then $s_i(x)$ has the following asymptotic
		\begin{align}
		s_i(x)\sim n\phi^{-1}\left(\frac{i}{n}\right)
		\end{align}
		where $\phi(x)=(1+x^{\alpha+1})^{\frac{1}{\alpha+1}}-x$.
	\end{lemma}
	
	\begin{proof}
		Let $l,k>0$ be such that $n(k-l)=i$ and
		\begin{align}
		\sum_{j=nl}^{nk} f(j)\le x<\sum_{j=nl+1}^{nk+1} f(j)
		\end{align}
		%not unique but almost unique to the point it doesnt matter
		Clearly it follows that $l$ and $k$ are unique up to producing the same range of integer indices in the sum. Using Lemma~\ref{sum f range asymp} and Lemma~\ref{sum f Asymp}
		\begin{align}
		\frac{1}{\alpha+1}L(n)n^{\alpha+1}(k^{\alpha+1}-l^{\alpha+1})\sim \sum_{j=nl}^{nk} f(j)\sim x\sim \sum_{j=1}^{n} f(j) \sim \frac{n^{\alpha+1}L(n)}{\alpha+1}.
		\end{align}
		Thus
		\begin{align}
			\frac{n^{\alpha+1}L(n)}{\alpha+1}(k^{\alpha+1}-l^{\alpha+1})&\sim \frac{n^{\alpha+1}L(n)}{\alpha+1}\\
			k^{\alpha+1}-l^{\alpha+1}&\sim 1\\
			k-l&\sim (1+l^{\alpha+1})^{\frac{1}{\alpha+1}}-l\\
			i&\sim n((1+l^{\alpha+1})^{\frac{1}{\alpha+1}}-l)\\
			ln&\sim n\phi^{-1}(\frac{i}{n}).
		\end{align}
		Lemma \ref{phi integral} proves that $\phi^{-1}$ is well defined. Since $s_i(x)$ counts the number of blocks whose sum is less than $x$ of length $i$, we get
		\begin{align}
		s_i(x) = nl\sim n\phi^{-1}(\frac{i}{n}).
		\end{align}
	\end{proof}
 
	\section{Case $\alpha>1$}\label{mainTheorem}
	\begin{lemma}\label{head's neglible}
		Let $\epsilon>0$ and $\alpha>1$ then 
		\begin{align}
		\sum_{i=1}^{n\epsilon} s_i(x)=O(n^2)\epsilon^{\frac{1}{2}(1-\frac{1}{\alpha})}.
		\end{align}
	\end{lemma}
	
	\begin{proof}
		Clearly $s_i(x)\le f^{-1}(\frac{x}{i})$. Regularly varying functions inverse is regularly varying with index $\frac{1}{\alpha}$ (see Theorem 1.5.12 of \cite{bingham1987regular} or Theorem~\ref{RVinverse} in the Appendix). Let $f^{-1}(x)=x^{\frac{1}{\alpha}}\tilde{L}(x)$.
		\begin{align}
		\sum_{i=1}^{n\epsilon} f^{-1}(\frac{x}{i})\sim  \int_{1}^{n\epsilon} f^{-1}(\frac{x}{t})dt=x^{\frac{1}{\alpha}}\int_{1}^{n\epsilon} t^{-\frac{1}{\alpha}}\tilde{L}(\frac{x}{t})dt
		\end{align}
		By Potter's bound
		\begin{align}
		\tilde{L}(\frac{x}{t})\le M\tilde{L}(\frac{x}{n})(\frac{n}{t})^{\delta},
		\end{align}
		where $M$ is the constant from Potter's bound and $\delta=\frac{1}{2}(1-\frac{1}{\alpha})$. Now we can bound the integral
		\begin{align}
			x^{\frac{1}{\alpha}}\int_{1}^{n\epsilon} t^{-\frac{1}{\alpha}}\tilde{L}(\frac{x}{t})dt&\le
			x^{\frac{1}{\alpha}}M\tilde{L}(\frac{x}{n})n^\delta\int_{1}^{n\epsilon} t^{-\frac{1}{\alpha}-\delta}\\
		\end{align}
		By definition
		\begin{align}
			f^{-1}(f(n))\sim n\\
			n\tilde{L}(n^\alpha L(n))L(n)^{1/\alpha}\sim n\\
			\tilde{L}(n^\alpha L(n))L(n)^{1/\alpha}\sim 1.
		\end{align}
		Thus
		\begin{align}
			x^{\frac{1}{\alpha}}M\tilde{L}(\frac{x}{n})n^\delta\int_{1}^{n\epsilon} t^{-\frac{1}{\alpha}-\delta}
			&\sim Mn^{2}L(n)^{\frac{1}{\alpha}}\tilde{L}(n^\alpha L(n))\frac{2\alpha}{\alpha-1}\epsilon^{\frac{1}{2}(1-\frac{1}{\alpha})}\\
			&=O(n^2)\epsilon^{\frac{1}{2}(1-\frac{1}{\alpha})}
		\end{align}		
	\end{proof}
	
	\begin{theorem}\label{BlockSumsforAlphaGr1}
		The function $S(x)$ has asymptotic $C(\alpha)n^2$, where 
		\begin{align}
		C(\alpha)=\frac{1}{2(\alpha+1)}
		\frac{\Gamma(\frac{1}{\alpha+1})\Gamma(\frac{\alpha-1}{\alpha+1})}{\Gamma(\frac{\alpha}{\alpha+1})}.
		\end{align}
		For $\alpha>1$.
	\end{theorem}
	
	\begin{proof}
		Let $\epsilon>0$. From the definition of $S$ and $s_i$ it follows that
		\begin{align}
		S(x)=\sum_{i=1}^{n} s_i(x).
		\end{align}
		Splitting the sum
		\begin{align}
		S(x)=\sum_{i=1}^{n\epsilon} s_i(x)+\sum_{i=n\epsilon}^{n} s_i(x).
		\end{align}
		Using lemma~\ref{head's neglible} we can control the first sum, so we will focus on the second sum. Using lemma~\ref{s asymp} we get
		\begin{align}
			\sum_{i=n\epsilon}^{n} s_i(x)&\sim \sum_{i=n\epsilon}^{n} n\phi^{-1}(\frac{i}{n})\\
			&=n\sum_{i=n\epsilon}^{n} \phi^{-1}(\frac{i}{n})\\
			&\sim n \int_{n\epsilon}^{n} \phi^{-1}(\frac{t}{n})dt\\
			&=n^2\int_{\epsilon}^{1} \phi^{-1}(t)dt.
		\end{align}
		We can bound $\phi^{-1}$, using the bound derived in Lemma~\ref{phi integral}
		\begin{align}
			\phi(x)&<x^{-\alpha}\\
			\phi(x^{-\frac{1}{\alpha}})&<x\\
			x^{-\frac{1}{\alpha}}&<\phi^{-1}(x)
		\end{align}
		Now we can bound
		\begin{align}
			\int_{0}^{\epsilon} \phi^{-1}(t)dt&=\epsilon\phi^{-1}(\epsilon)+\int_{\phi^{-1}(\epsilon)}^{\infty} \phi(t)dt\\
			&<\epsilon^{1-\frac{1}{\alpha}}+\phi^{-1}(\epsilon)^{1-\alpha}.
		\end{align}
		We get
		\begin{align}
			n^2\int_{\epsilon}^{1} \phi^{-1}(t)dt=n^2(\int_{0}^{1} \phi^{-1}(t)dt-\int_{0}^{\epsilon} \phi^{-1}(t)dt)\\
			>n^2(C(\alpha)-\epsilon^{1-\frac{1}{\alpha}}+\phi^{-1}(\epsilon)^{1-\alpha})
		\end{align}
		and
		\begin{align}
			n^2\int_{\epsilon}^{1} \phi^{-1}(t)dt<n^2C(\alpha).
		\end{align}
		Combining this and bound from Lemma~\ref{head's neglible} the Theorem follows.
	\end{proof}
	
	\section{Empirical Convergence of Asymptotic Formula}
	
	Figures~\ref{SVeffectsOnConvergence} and \ref{indexEffectsOnConvergence} show the impact of slowly varying part and index on convergence, respectively. We can observe that increasing the index, slows down the convergence and larger growth rate in the slowly varying part corresponds to slower convergence.
	
	The convergence is quite slow. For example $S(x)$ for $f(x)=x^3\log(x)$ at $x=10^{20}$ has still relative error of $\approx 1.4\%$ compared to the asymptotic formula and for $f(x)=x^3 \log^2(x)$ the relative error is $\approx 2.8\%$.
	
	These findings align with our analysis: the proofs rely on uniform convergence theorem which gives slower convergence for larger growth rates and the sum's asymptotic to integral has slower convergence for larger indices.
	
	\section{Conclusion}
	The main theorem gives asymptotic for number of block sums below $x$. It remains open question what happens in the case $\alpha\le 1$. Future work could refine the result by dropping assumptions or investigating a different class of functions. The result may have applications in additive and analytic number theory.
	
	
	\bibliographystyle{plain}
	\bibliography{references}
\end{document}